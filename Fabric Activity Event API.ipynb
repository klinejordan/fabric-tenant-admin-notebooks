{"cells":[{"cell_type":"markdown","id":"e9efd966-8955-41ee-ab66-c1c6966e893f","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fabric Activity Event API\n","This notebook calls the [Power BI Activity Events API](https://learn.microsoft.com/en-us/rest/api/power-bi/admin/get-activity-events) to maintain the following files in a Lakehouse:\n","- A fact table in Delta format containing a subset of fields from each activity record, appended to each time the notebook is run\n","- A .json file containing a complete record of all the results from the Activity Events API for each day the notebook is run\n","\n","**Important Notes:**\n","- **Schedule this notebook to run nightly after midnight UTC** to capture all activities every day\n","\n","**Prerequisites:**\n","- A Power BI Admin must turn on the \"[Allow Service Principals to use read-only Admin APIs](https://learn.microsoft.com/en-us/power-bi/enterprise/read-only-apis-service-principal-authentication)\" feature. \n","- If the Allow Service Principals to use read-only Admin APIs switch is turned on _and limited to specific security groups_, the service principal used to acquire the bearer token must be in one of the group(s) allowed to use the read-only Admin APIs\n","- The service principal used to acquire the bearer token must have Tenant.Read.All or Tenant.ReadWrite.All permissions\n","- Multi-Factor Authentication (MFA) must be disabled on the service account/user used to acquire the API bearer tokens\n","- **This notebook assumes you are storing service principal Client ID and Client Secret in an Azure Key Vault**. Fabric will retrieve them securely, use them during Notebook runs as obscure the values in logs and cell outputs. Avoid hard-coding service principal secrets in Notebooks\n","\n","**Future Work:**\n","- Support for band-aids with known issues with the Activity Events API, including:\n","    - Analyzed By External Application activities don't show the Workspace"]},{"cell_type":"markdown","id":"6a761046-aae7-4b34-b9b2-09f3cfa79fb5","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Define parmaters for Key Vault, Fabric base URIs, write mode, time zone, ignored activities and Lakehouse File directories"]},{"cell_type":"code","execution_count":null,"id":"88569982-ef34-4172-bb18-3ffdf5b3b5e1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["# Name of the Key Vault and names of the secrets\n","nameOfKeyVault = 'xxx' # Name of the Key Vault\n","tenantId_SecretName = 'xxx'   # Tenant ID secret name\n","clientId_SecretName = 'xxx'   # Name for Client ID of Service Principal\n","clientSecret_SecretName = 'xxx' # Name for Client Secret of Service Principal\n","\n","# Base URLS for Power BI and Fabric\n","powerBIAPIBaseUri = 'https://api.powerbi.com/v1.0/myorg/'\n","fabricAPIBaseUri = 'https://api.fabric.microsoft.com/v1/'\n","\n","# Time zone for UTC converstion - see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for list of possible values\n","timeZone = 'America/New_York'\n","\n","# Set recalculateTable = 'Overwrite' if you want to completely repopulate the table from the archived json files, 'Append' if you want to append yesterday's results only\n","writeMode = 'Append'\n","\n","# list of activities to ignore - add/remove as necessary for your requirements but in my experience these tend to make a lot of noise in the API and provide little value\n","ignoreActivities = ['GenerateCustomVisualAADAccessToken','GenerateCustomVisualWACAccessToken','GenerateDataflowSasToken','GenerateScreenshot']\n","\n","# folder/file names\n","nameOfActivitiesFileFolder = 'Files/ActivityEventsAPI/'\n","nameOfActivitiesDeltaTable = 'Activities'"]},{"cell_type":"markdown","id":"4bb7e9dd-1dac-4cb9-b645-79b9c6360d67","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Import libraries and set Spark session settings"]},{"cell_type":"code","execution_count":null,"id":"452f157d-f390-43fd-a11a-e48b56deaf3b","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql import DataFrame\n","from delta.tables import *\n","import requests \n","import json\n","from datetime import datetime, date, timedelta, timezone "]},{"cell_type":"markdown","id":"d9b6fc02-8453-4413-b471-946e7b977d93","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Define functions to get bearer token, make API calls and recreate/appent to Activities table"]},{"cell_type":"code","execution_count":null,"id":"47db3831-1da8-49d2-9957-e5133f8e73fd","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def fxGetFabricBearerToken():\n","    keyvault = f'https://{nameOfKeyVault}.vault.azure.net/'\n","    # Begin don't hard code this stuff\n","    tenant_id = mssparkutils.credentials.getSecret(keyvault,tenantId_SecretName)\n","    client_id = mssparkutils.credentials.getSecret(keyvault,clientId_SecretName)\n","    client_secret = mssparkutils.credentials.getSecret(keyvault,clientSecret_SecretName)\n","    # End don't hard code this stuff\n","    url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/token'\n","    data = f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource=https://analysis.windows.net/powerbi/api'  \n","    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n","    response = requests.post(url, headers=headers, data=data)\n","    return response.json()[\"access_token\"]\n","\n","def fxRequestWithRetry(method, url, data, num_retries=3, success_list=[200, 202, 404], **kwargs):\n","    bearer_token = fxGetFabricBearerToken()\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {bearer_token}'}\n","    for i in range(num_retries):\n","        try:\n","            response = requests.request(method, url, headers=headers, data=data, **kwargs)\n","            if response.status_code in success_list:\n","                ## Return response if successful\n","                return response\n","            ## Captures the 500 requests in an hour limit\n","            if response.status_code == 429 and response.headers.get('Retry-After',None) is not None:\n","                waitTime = int(response.headers['Retry-After'])\n","                print(f'Hit the 500 requests per hour rate limit - waiting {str(waitTime)} seconds until next retry')\n","                time.sleep(waitTime)\n","            ## Captures the 16 simultaneous requests limit\n","            if response.status_code == 429 and response.headers.get('Retry-After',None) is None:\n","                waitTime = 120\n","                print(f'Hit the 16 simultaneous requests limit - waiting {str(waitTime)} seconds until next retry')\n","                time.sleep(waitTime)\n","        except requests.exceptions.ConnectionError:\n","            pass\n","    return None\n","\n","def fxCalculateActivitiesTable(df):\n","    activities = df.withColumn('WorkspaceID',coalesce(col('WorkspaceId'),col('FolderObjectId')))\\\n","    .withColumn('ReportID',coalesce(col('AppReportId'),col('ReportId')))\\\n","    .withColumn('ActivityDateUTC',col('CreationTime').cast('date'))\\\n","    .withColumn('ActivityDatetimeUTC',col('CreationTime').cast('timestamp'))\\\n","    .withColumn('ActivityDateLocal',from_utc_timestamp(col('CreationTime'),timeZone).cast('date'))\\\n","    .withColumn('ActivityDatetimeLocal',from_utc_timestamp(col('CreationTime'),timeZone).cast('timestamp'))\\\n","    .filter(~col('Activity').isin(ignoreActivities))\\\n","    .select(col('Id').alias('ActivityID'), \n","        col('UserId').alias('UserID'), \n","        col('UserAgent'),\n","        col('ClientIP'),\n","        col('Activity'),\n","        col('ActivityDatetimeLocal'),\n","        col('ActivityDateLocal'),\n","        col('DistributionMethod'),\n","        col('ConsumptionMethod'),\n","        col('ItemName'),\n","        col('CapacityId').alias('CapacityID'),\n","        col('DomainId').alias('DomainID'),\n","        col('WorkspaceID'),\n","        col('AppId').alias('AppID'),\n","        col('ArtifactId').alias('ArtifactID'),\n","        col('ObjectId').alias('ObjectID'),\n","        col('ReportID'),\n","        col('DatasetId').alias('SemanticModelID'),\n","        col('DataflowId').alias('DataflowID'),\n","        col('DatamartId').alias('DatamartID'),\n","        col('DashboardId').alias('DashboardID'),\n","        col('TileId').alias('DashboardTileID'))\\\n","    .fillna({'DomainID':'00000000-0000-0000-0000-00000000',\n","        'CapacityID':'00000000-0000-0000-0000-00000000',\n","        'WorkspaceID':'00000000-0000-0000-0000-00000000',\n","        'DistributionMethod':'N/A',\n","        'ConsumptionMethod':'N/A',\n","        'ItemName':'N/A',\n","        'AppID':'00000000-0000-0000-0000-00000000',\n","        'ArtifactID':'00000000-0000-0000-0000-00000000',\n","        'ObjectID':'00000000-0000-0000-0000-00000000',\n","        'ReportID':'00000000-0000-0000-0000-00000000',\n","        'SemanticModelID':'00000000-0000-0000-0000-00000000',\n","        'DataflowID':'00000000-0000-0000-0000-00000000',\n","        'DatamartID':'00000000-0000-0000-0000-00000000',\n","        'DashboardID':'00000000-0000-0000-0000-00000000',\n","        'DashboardTileID':'00000000-0000-0000-0000-00000000'})\\\n","    .write.mode(writeMode).format(\"delta\").save(\"Tables/\" + nameOfActivitiesDeltaTable)\n","    if writeMode == 'Overwrite':\n","        return print(f'{nameOfActivitiesDeltaTable} overwritten from archived JSON files')\n","    if writeMode == 'Append':\n","        return print(f'Appended restults from {str(yesterday)} into {nameOfActivitiesDeltaTable}')"]},{"cell_type":"markdown","id":"c171fa78-b8f5-4d6b-9c12-1197d7d38410","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Define schema of Activities payload"]},{"cell_type":"code","execution_count":null,"id":"ba13ae6c-c110-46ca-a9de-a5734ff39d7e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## define initial dataframe schema\n","schema = StructType([\n","  StructField(\"Id\",StringType(),True),\n","  StructField(\"RequestId\",StringType(),True),\n","  StructField(\"CreationTime\",StringType(),True),\n","  StructField(\"UserId\",StringType(),True),\n","  StructField(\"UserAgent\",StringType(),True),\n","  StructField(\"ClientIP\",StringType(),True),\n","  StructField(\"Activity\",StringType(),True),\n","  StructField(\"DistributionMethod\",StringType(),True),\n","  StructField(\"ConsumptionMethod\",StringType(),True),\n","  StructField(\"ItemName\",StringType(),True),\n","  StructField(\"CapacityId\",StringType(),True),\n","  StructField(\"DomainId\",StringType(),True),\n","  StructField(\"WorkspaceId\",StringType(),True),\n","  StructField(\"FolderObjectId\",StringType(),True),\n","  StructField(\"DatasetId\",StringType(),True),\n","  StructField(\"ReportId\",StringType(),True),\n","  StructField(\"ArtifactId\",StringType(),True),\n","  StructField(\"ObjectId\",StringType(),True),\n","  StructField(\"AppId\",StringType(),True),\n","  StructField(\"AppReportId\",StringType(),True),\n","  StructField(\"DataflowId\",StringType(),True),\n","  StructField(\"DatamartId\",StringType(),True),\n","  StructField(\"DashboardId\",StringType(),True),\n","  StructField(\"TileId\",StringType(),True)\n","])"]},{"cell_type":"markdown","id":"e653b75d-57a6-43bd-beb5-df4fe0e12c51","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Define URI for Activity Events API based on current date in UTC"]},{"cell_type":"code","execution_count":null,"id":"ee70a21f-c8ae-42f3-8ce4-a931e822d838","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## define initial batch number (Activity Events API chunks responses)\n","batch_number = 1\n","## define yesterday\n","yesterday = date.today() - timedelta(days = 1)\n","## create string for yesterday in format Activity Events API expects\n","yesterday_uri = yesterday.strftime(\"%Y-%m-%d\")\n","## create string for yesterday in format for file name\n","yesterday_file = yesterday.strftime(\"%Y%m%d\")\n","## create full Uri for Activity Events API\n","activities_Uri = powerBIAPIBaseUri + \"admin/activityevents?startDateTime='\" + yesterday_uri + \"T00:00:00'&endDateTime='\" + yesterday_uri + \"T23:59:59'\"\n","print(f'Using {activities_Uri} to retrieve activities')"]},{"cell_type":"markdown","id":"bedbe7a7-e601-4796-a637-70baf845e8d2","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Get all batches of yesterday's activities and store in a dataframe"]},{"cell_type":"code","execution_count":null,"id":"506e9c97-357b-4fc9-b9e8-077f92152fb1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["print(f'Starting to retrieve activities for {str(yesterday)}')\n","## get first batch of activity data\n","data = fxRequestWithRetry('GET', activities_Uri,'')\n","## set the continuation uri for next batch\n","contUri = data.json()['continuationUri']\n","## create dict with the batches' activities\n","j = data.json()['activityEventEntities']\n","## create data frame for delta table\n","df = spark.createDataFrame(spark.sparkContext.parallelize(j),schema)\n","print(f'Batch {str(batch_number)} completed - {str(len(j))} activities found')\n","## do until last batch is completed\n","while contUri is not None: \n","    ## increment batch number       \n","    batch_number = batch_number + 1\n","    ## get next batch of activity data\n","    data_cont = fxRequestWithRetry('GET', contUri,'')\n","    ## set the continuation uri for next batch\n","    contUri = data_cont.json()['continuationUri']\n","    ## append json\n","    d = data_cont.json()['activityEventEntities'] \n","    j.extend(d)\n","    ## create data frame for delta table\n","    df_cont = spark.createDataFrame(spark.sparkContext.parallelize(d),schema)    \n","    ## union dataframes\n","    df = df.union(df_cont)\n","    print(f'Batch {str(batch_number)} completed - {str(len(d))} activities found')\n","print('All Batches Retrieved and appended')"]},{"cell_type":"markdown","id":"8cd71caa-b82e-45e2-b82d-9b8e7dcd127f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Check if Activities Lakehouse folder exists and create if it doesn't; then write activity batches to JSON file in Lakehouse"]},{"cell_type":"code","execution_count":null,"id":"a3712518-0d11-4cbe-bc8c-ac595a486f73","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["mssparkutils.fs.mkdirs(nameOfActivitiesFileFolder)\n","with open(f'/lakehouse/default/{nameOfActivitiesFileFolder}Activities_{yesterday_file}.json', 'w') as f:\n","    f.write(json.dumps(j))\n","    f.close()\n","    print(f'/lakehouse/default/{nameOfActivitiesFileFolder}Activities_{yesterday_file}.json created')"]},{"cell_type":"markdown","id":"2e90633d-095d-4e1a-9443-b437ebc552ef","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["#### Overwrite or append to Activities table"]},{"cell_type":"code","execution_count":null,"id":"83ba0d71-4ea9-4138-927a-a3eb5ef75b81","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["archive_df = spark.read.json(f'{nameOfActivitiesFileFolder}/*.json',schema = schema)\n","if writeMode == 'Overwrite':\n","    fxCalculateActivitiesTable(archive_df) \n","if writeMode == 'Append': \n","    fxCalculateActivitiesTable(df)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"ff416cba-17a5-4d5e-9aaf-f4eca104bcad","default_lakehouse_name":"FabricReporting","default_lakehouse_workspace_id":"a981acaf-e835-4c25-a629-27d3501438e5","known_lakehouses":[{"id":"ff416cba-17a5-4d5e-9aaf-f4eca104bcad"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
