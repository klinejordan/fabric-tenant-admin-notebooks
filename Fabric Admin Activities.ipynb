{"cells":[{"attachments":{},"cell_type":"markdown","id":"0c7763bc-8405-4f4a-bccf-eb161403c30d","metadata":{},"source":["## Fabric Activities\n","This notebook calls the [Power BI Activity Events API](https://learn.microsoft.com/en-us/rest/api/power-bi/admin/get-activity-events) to create the following items in a Lakehouse:\n","- A fact table in Delta format containing a subset of fields from each activity record, appended to each time the notebook is run\n","- A .json file containing a complete record of all the results from the Activity Events API for each day the notebook is run\n","\n","**Important Notes:**\n","- **Schedule this notebook to run nightly after midnight UTC** to capture all activities every day\n","- A Power BI Admin must turn on the \"[Allow Service Principals to use read-only Admin APIs](https://learn.microsoft.com/en-us/power-bi/enterprise/read-only-apis-service-principal-authentication)\" feature. \n","- If the Allow Service Principals to use read-only Admin APIs is turned on _and limited to specific security groups_, the service principal used to acquire the bearer token must be in one of the group(s) allowed to use the read-only Admin APIs\n","- The service principal used to acquire the bearer token must have Tenant.Read.All or Tenant.ReadWrite.All permissions\n","- Multi-Factor Authentication (MFA) must be disabled on the service account/user used to acquire the API bearer tokens\n","\n","**Future Work:**\n","- Support for band-aids with known issues with the Activity Events API, including:\n","    - Workspace IDs are sometimes shows as \"FolderObjectId\"\n","    - Analyzed By External Application activities don't show the Workspace ID"]},{"attachments":{},"cell_type":"markdown","id":"ff5812a2-9df3-42a2-b353-2bda481eba1b","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"id":"b34719dc-0542-44e5-95aa-637238bec2eb","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from datetime import date, timedelta\n","import requests \n","import json"]},{"attachments":{},"cell_type":"markdown","id":"c3a65994-57a0-438e-b70c-ecf5a8df1130","metadata":{},"source":["### Define variables"]},{"cell_type":"code","execution_count":null,"id":"dce0a2b7-8cd0-42a3-b3fc-b1ea5dcecc1b","metadata":{},"outputs":[],"source":["# Name of the Key Vault\n","nameOfKeyVault = 'powerbi-admin-keyvault' # Name of the Key Vault\n","\n","# Names of the secrets saved in Key Vault\n","tenantIdSecretName = 'xxxxxxxxxxxxx' # Name for Tenant ID\n","clientIdSecretName = 'xxxxxxxxxxxxx'   # Name for Client ID of Service Principal\n","clientSecretSecretName = 'xxxxxxxxxxxxx' # Name for Client Secret of Service Principal\n","\n","# Base URLS for Power BI and Key Vault\n","pbiUri = 'https://api.powerbi.com/v1.0/myorg/'\n","keyVaultUri = f'https://{nameOfKeyVault}.vault.azure.net/'\n","\n","# set to 1 if you want to retain a raw .json file of the activities in your lakehouse - set to 0 if you want to skip that step\n","saveJsonFile = 1\n","\n","# list of activities to ignore when creating Delta table - add/remove as necessary for your requirements\n","ignoreActivities = ['GenerateCustomVisualAADAccessToken','GenerateCustomVisualWACAccessToken','GenerateDataflowSasToken']\n","\n","# folder/file names\n","nameOfFileFolder = 'Activities'\n","nameofDeltaTable = 'Activities'"]},{"attachments":{},"cell_type":"markdown","id":"7cd44c79-e5bf-46a2-9127-74064ae05d48","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Define functions to get key vault secrets, API bearer token and JSON responses"]},{"cell_type":"code","execution_count":null,"id":"61959c0d-307a-4519-a586-3c096f85035e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def get_bearer_token():\n","    tenant_id = mssparkutils.credentials.getSecret(keyVaultUri,tenantIdSecretName)\n","    client_id = mssparkutils.credentials.getSecret(keyVaultUri,clientIdSecretName)\n","    client_secret = mssparkutils.credentials.getSecret(keyVaultUri,clientSecretSecretName)\n","    url = \"https://login.microsoftonline.com/\" + tenant_id + \"/oauth2/token\"\n","    data = \"grant_type=client_credentials&client_id=\" + client_id + \"&client_secret=\" + client_secret + \"&resource=https://analysis.windows.net/powerbi/api\"  \n","    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n","    response = requests.post(url, headers=headers, data=data)\n","    return response.json()[\"access_token\"]\n","\n","def get_response_json(fullurl, method, data , payload_object):\n","    bearer_token = get_bearer_token()\n","    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {bearer_token}'}\n","    response = requests.request(method, fullurl, headers=headers, data=data)\n","    if payload_object == \"\":\n","        return response.json()\n","    else:\n","        return response.json()[payload_object]"]},{"attachments":{},"cell_type":"markdown","id":"52edd52a-c3a7-490f-8801-e2b1c209bcbc","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Get yesterday's activity, run until continuation token is empty, save raw json file and insert into delta table"]},{"cell_type":"code","execution_count":null,"id":"e19e4e22-b347-43ef-a1ce-7845883f2a5d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## define initial batch number (Activity Events API chunks responses)\n","batch_number = 1\n","## define yesterday\n","yesterday = date.today() - timedelta(days = 1)\n","## create string for yesterday in format Activity Events API expects\n","yesterday_uri = yesterday.strftime(\"%Y-%m-%d\")\n","## create string for yesterday in format for file name\n","yesterday_file = yesterday.strftime(\"%Y%m%d\")\n","## create full Uri for Activity Events API\n","activites_Uri = pbiUri + \"admin/activityevents?startDateTime='\" + yesterday_uri + \"T00:00:00'&endDateTime='\" + yesterday_uri + \"T23:59:59'\"\n","\n","## define initial dataframe schema\n","schema = StructType([\n","    StructField(\"Id\",StringType(),True),\n","    StructField(\"RequestId\",StringType(),True),\n","    StructField(\"CreationTime\",StringType(),True),\n","    StructField(\"UserId\",StringType(),True),\n","    StructField(\"UserAgent\",StringType(),True),\n","    StructField(\"ClientIP\",StringType(),True),\n","    StructField(\"Activity\",StringType(),True),\n","    StructField(\"DistributionMethod\",StringType(),True),\n","    StructField(\"ConsumptionMethod\",StringType(),True),\n","    StructField(\"ItemName\",StringType(),True),\n","    StructField(\"CapacityId\",StringType(),True),\n","    StructField(\"WorkspaceId\",StringType(),True),\n","    StructField(\"FolderObjectId\",StringType(),True),\n","    StructField(\"DatasetId\",StringType(),True),\n","    StructField(\"ReportId\",StringType(),True),\n","    StructField(\"ArtifactId\",StringType(),True),\n","    StructField(\"AppId\",StringType(),True),\n","    StructField(\"AppReportId\",StringType(),True),\n","    StructField(\"DataflowId\",StringType(),True),\n","    StructField(\"DashboardId\",StringType(),True),\n","    StructField(\"TileId\",StringType(),True)\n","  ])\n","\n","## get first batch of activity data\n","data = get_response_json(activites_Uri,\"GET\", \"\", \"\")\n","\n","## set the continuation uri for next batch\n","contUri = data['continuationUri']\n","\n","## create dict with the batches' activities\n","j = data['activityEventEntities']\n","\n","## create data frame for delta table\n","df = spark.createDataFrame(spark.sparkContext.parallelize(data['activityEventEntities']),schema)\n","\n","## print success\n","print(\"Batch \" + str(batch_number) + \" completed - \" + str(len(data['activityEventEntities'])) + ' activities found' )\n","\n","## do until last batch is completed\n","while contUri is not None: \n","\n","        ## increment batch number       \n","        batch_number = batch_number + 1\n","\n","        ## get next batch of activity data\n","        data_cont = get_response_json(contUri,\"GET\", \"\", \"\")\n","        \n","        ## set the continuation uri for next batch\n","        contUri = data_cont['continuationUri']\n","\n","        ## append json \n","        j.extend(data_cont['activityEventEntities'])\n","    \n","        ## create data frame for delta table\n","        df_cont = spark.createDataFrame(spark.sparkContext.parallelize(data_cont['activityEventEntities']),schema)    \n","    \n","        ## union dataframes\n","        df = df.union(df_cont)\n","\n","        ## print success\n","        print(\"Batch \" + str(batch_number) + \" completed - \" + str(len(data_cont['activityEventEntities'])) + ' activities found' )\n","\n","## write to json file in default lakehouse\n","if saveJsonFile == 1:\n","    with open(\"/lakehouse/default/Files/\" + nameOfFileFolder + \"/Activities\" + \"_\" + yesterday_file + \".json\", \"w\") as f:\n","            f.write(json.dumps(j))\n","            f.close()\n","    print(\"/lakehouse/default/Files/\" + nameOfFileFolder + \"/Activities\" + \"_\" + yesterday_file + \".json created\")        \n","else: \n","    None\n","\n","## filter out some activities before writing to delta table\n","df = df.filter(~df[\"Activity\"].isin(ignoreActivities))\n","\n","## write to delta table\n","if spark.catalog.tableExists(nameofDeltaTable):\n","    writeToLake = df.write.mode(\"append\").format(\"delta\").save(\"Tables/\" + nameofDeltaTable)\n","else: \n","    writeToLake = df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + nameofDeltaTable)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"90082ddd-3a48-4706-a72a-6733248f4e51","default_lakehouse_name":"PowerBIReporting","default_lakehouse_workspace_id":"d94992fd-1d38-4641-8eda-9d5d7db342da","known_lakehouses":[{"id":"90082ddd-3a48-4706-a72a-6733248f4e51"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
